# -*- coding: utf-8 -*-
"""Section4_Project_AI_06_강지호

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10bJWe-2RYpsEXQ_COIHyN7ufq5F1p6yr
"""

!pip install kaggle

from google.colab import files

file = files.upload()

ls -1ha kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
# Permission Warning 이 일어나지 않도록 
!chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d rounakbanik/the-movies-dataset

! ls

! unzip the-movies-dataset.zip

import pandas as pd
import numpy as np

rating_raw = pd.read_csv('/content/ratings.csv')
rating_raw

movies_metadata = pd.read_csv('/content/movies_metadata.csv', usecols=['adult', 'genres', 'id', 'original_language', 'original_title', 'overview'])
# adult는 9개뿐, 제목에는 영어 외의 한국어, 중국어등이 포함되어 있음.
movies_metadata

movies_metadata.info()

# id에 int가 아닌 행 제거
non_int = []
for i in range(len(movies_metadata.id)):
  try:
    int(movies_metadata.id[i])
    pass
  except:
    non_int.append(i)
movies_metadata.drop(labels=non_int, axis=0, inplace=True)

# id 를 int로 변경
movies_metadata['id'] = pd.to_numeric(movies_metadata['id'])

# overview에 null인 행 제거
movies_metadata = movies_metadata[movies_metadata.overview.notnull()]

# 전체적으로 중복 행 제거
movies_metadata.drop_duplicates()

print(movies_metadata.info())
movies_metadata.head()

rating_raw

rating_raw.isnull().sum()

df = pd.merge(rating_raw, movies_metadata, how='left', left_on='movieId', right_on='id')
df.dropna(subset=['userId', 'movieId', 'id', 'overview'], inplace=True)
df

df.info()

df.isnull().sum()

df.sort_values('rating')

"""#### tokenizer"""

# 줄거리 영어를 전처리 해보자
import re
import spacy
from spacy.tokenizer import Tokenizer

nlp = spacy.load("en_core_web_sm")
tokenizer = Tokenizer(nlp.vocab)

eda = []
for row in tokenizer.pipe(movies_metadata.overview):
  before = re.sub(r'\n', ' ', row.text)
  desc = re.sub(r'[^a-z0-9 ]', "", before.lower())
  eda.append(desc)

movies_metadata['token'] = eda
movies_metadata

# 줄거리 토큰화 시켜서 padding 300차원으로 맞춰주기
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(3000)
tokenizer.fit_on_texts(movies_metadata.token)
print('tokenizer word index:', len(tokenizer.word_index)) # 빈번하게 사용되는 단어가 1000개인가?

# input의 각 행 단어를 인덱스로 변환
token_encoded = tokenizer.texts_to_sequences(movies_metadata.token)

print('movies_metadata.token의 각 행 중 최대 길이 :', max(len(sent) for sent in movies_metadata.token))
max_len = max(len(sent) for sent in token_encoded)
print('token_encoded 각 행 중 최대 길이 max_len:', max_len)

print(f'Mean length of movies_metadata.token: {np.mean([len(sent) for sent in movies_metadata.token], dtype=int)}')
print(f'Mean length of token_encoded: {np.mean([len(sent) for sent in token_encoded], dtype=int)}')

from tensorflow.keras.preprocessing.sequence import pad_sequences

token_padded = pad_sequences(token_encoded, maxlen=300)

# # 아래 다른 방법을 사용하면 이거랑 밑에 셀 실행해봐
# import gensim.downloader as api

# wv = api.load('word2vec-google-news-300')

# from tensorflow.keras.preprocessing.sequence import pad_sequences

# # 각 행 인덱스의 길이를 150으로 통일 (pad_sequence, maxlen =150)
# token_padded = pad_sequences(token_encoded, maxlen=300)

# # 임베딩 가중치 행렬 틀 만들기
# vocab_size = len(tokenizer.word_index) + 1
# print('vocab_size:', vocab_size)

# embedding_matrix = np.zeros((vocab_size, 300))
# print('embedding matrix shape:', np.shape(embedding_matrix))

# # tokenizer에 저장되어 있는 임베딩 벡터만 가져와서 위의 임베딩 가중치 행렬에 값 넣기
# def get_vector(word):
#     """
#     해당 word가 word2vec에 있는 단어일 경우 임베딩 벡터를 반환
#     """
#     if word in wv:
#         return wv[word]
#     else:
#         return None
 
# for word, i in tokenizer.word_index.items():
#     temp = get_vector(word)
#     if temp is not None:
#         embedding_matrix[i] = temp

# token_with_id = pd.DataFrame(token_padded)
# token_with_id['id'] = movies_metadata.id.astype(int)
# token_with_id

token_padded = pd.DataFrame(token_padded)
token_padded['id'] = movies_metadata.id.astype(int)
token_padded

"""### 모델 구축"""

from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
import tensorflow as tf

np.random.seed(42)
tf.random.set_seed(42)

df_9, test_01 = train_test_split(df, test_size=0.01, random_state = 100)

df_9.shape, test_01.shape

train, test = train_test_split(test_01, test_size=0.2, random_state = 100)

train.shape, test.shape

train.sort_values('id', inplace=True)
test.sort_values('id', inplace=True)

train.head()

token_padded.head()

number_of_unique_user = len(test_01.userId.unique())
number_of_unique_movie_id = len(test_01.id)
number_of_unique_user, number_of_unique_movie_id

from tensorflow.keras.layers import Dense, Embedding, Flatten, GlobalAveragePooling1D, LSTM, Input, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model

max_featues = 200000
# user_id
user_input = Input(shape=(1,), name='user_input_layer')
user_embedding_layer = Embedding(max_featues, 64, name = 'user_embedding_layer')
user_vector_layer = Flatten(name='user_vector_layer')

# # overview_id
# overview_input = Input(shape=(300,), name='overview_input_layer')
# overview_embedding_layer = Embedding(max_featues, 128, name = 'overview_embedding_layer')
# overview_vector_layer = LSTM(128)
'''
다른 방법
model = Sequential()
model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False))
model.add(GlobalAveragePooling1D()) # 입력되는 단어 벡터의 평균을 구하는 함수입니다.
'''

# movie_id
movie_input = Input(shape=(1,), name='movie_input_layer')
movie_embedding_layer = Embedding(max_featues, 64, name = 'movie_embedding_layer')
movie_vector_layer = Flatten(name='movie_vector_layer')



concate_layer = Concatenate()

dense_layer1 = Dense(128, activation='relu')
dense_layer2 = Dense(64, activation='relu')
dense_layer3 = Dense(32, activation = 'relu')
result_layer = Dense(1)


'''
쌓기
'''
# user_id
user_embedding = user_embedding_layer(user_input)
user_vector = user_vector_layer(user_embedding)

# # overview_id
# overview_embedding = overview_embedding_layer(overview_input)
# overview_vector = overview_vector_layer(overview_embedding)

# movie_id
movie_embedding = movie_embedding_layer(movie_input)
movie_vector = movie_vector_layer(movie_embedding)


concat = concate_layer([user_vector, movie_vector])
dense1 = dense_layer1(concat)
dense2 = dense_layer2(dense1)
dense3 = dense_layer3(dense2)



result = result_layer(dense3)

model = Model(inputs=[user_input,movie_input], outputs=result)

model.summary()

model.summary()

train.head()

train.userId

"""#### input data 준비하기"""

train = train.sort_values('id')

train_user_input = train.userId
train_movie_input = train.id

train_movie_input

train_overview_input = pd.DataFrame(columns = token_padded.columns[:-1])
for i in train_movie_input:
  k = token_padded[token_padded.id == i].drop('id', axis=1)
  train_overview_input = train_overview_input.append(k)
train_overview_input

train_user_input.shape, train_movie_input.shape, train_overview_input.shape

ary = [token_padded[token_padded.id == i].drop('id', axis=1) for i in train_movie_input]
ary_df = pd.DataFrame(ary, columns = range(300))
ary_df

token_padded.values.shape

test = test.sort_values('id')

test_user_input = test.userId
test_movie_input = test.id

test_movie_input

test_overview_input = pd.DataFrame(columns = token_padded.columns[:-1])
for i in test_movie_input:
  k = token_padded[token_padded.id == i].drop('id', axis=1)
  test_overview_input = test_overview_input.append(k)
test_overview_input

y_train = np.array(train.rating)
y_test = np.array(test.rating)

model.compile(loss = 'mse', optimizer='adam', metrics=['mse'])

tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)

type(train_user_input), type(train_overview_input), type(train_movie_input), type(y_train)



history = model.fit([train_user_input, train_movie_input], y_train, epochs=20, verbose=1)

from matplotlib import pyplot as plt

plt.plot(history.history['loss'])
plt.xlabel('epochs')
plt.ylabel('mse')

model.evaluate([test_user_input, test_movie_input], y_test)

from sklearn.metrics import r2_score

predictions = model.predict([test_user_input, test_movie_input])

f2_y_predict = r2_score(y_test, predictions)
print("R2 :", f2_y_predict)

